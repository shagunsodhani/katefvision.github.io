## Markov Decision Processes

* Concept of agent, state, action, environment and reward.
* Agent should be able to measure its success explicitly.

* Limitations
	* Can not be used when experiements take a lot of time or failure cases have a heavy cost involved.

* Markov Decision Process

	* Components
		* S - Set of states
		* A - Set of actions
		* T - Transition probability
		* r - Reward function
		* gamma - discount factor

	* Markov Property - Future is independent of the the past, given the present.

	* Model based vs Model free
		* Model based: State-action dynamics are either known or modelled/estimated
		* Model free: State-actions dynamics are not known

	* Return: Discounted sum of rewards

	* Infinite vs finite horizon

	* Policy:
		* distribution over actions given state
		* defines the behaviour of the agent
		* MDP policies are time-independent(stationary)

	* Prediction Vs Control
		* Prediction - Given policy, compute the value function for state and state-actions
		* Control - Estimate the value function for state and state-actions for the optimal policy

	* State-value function of an MDP - expected return starting from state s, and then following the policy
	* Action-value function of an MDP - expected return starting from state s, taking action a, and then following policy
	* Optimal state-value function - maximum value function over all policies
	* Optimal action-value function - maximum action-value function over all policies

	* Bellman expectation value
		* Representation as a linear system of equations

	* Optimal Policy
	* Mapping optimal state value function to optimal policy
	* Mapping optimal action value function to optimal policy
