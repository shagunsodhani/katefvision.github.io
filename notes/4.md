## Planning in Markov Decision Processes

* Markov Reward Process - MDP with a fixed policy

* Iterative methods
	* Bellman Equation
	* Backup Operation
	* Sweep

* Contraction Mapping Theorem

* Iterative Policy Evaluation === Policy Iteration

* Policy Iteration vs Value Iteration

* Asynchronous Dynamic Programming
	* In-place - Use only one set of variables
	* Prioritized Sweeping - Use the magnitude of Bellman error to decide which state to update
	* Real-time - Update only states which are relevant to the agent

* Approximate Dynamic Programming
	* Linear Programming


* Concept of agent, state, action, environment and reward.
* Agent should be able to measure its success explicitly.

* Limitations
	* Can not be used when experiements take a lot of time or failure cases have a heavy cost involved.

* Markov Decision Process

	* Components
		* S - Set of states
		* A - Set of actions
		* T - Transition probability
		* r - Reward function
		* gamma - discount factor

	* Markov Property - Future is independent of the the past, given the present.

	* Model based vs Model free
		* Model based: State-action dynamics are either known or modelled/estimated
		* Model free: State-actions dynamics are not known

	* Return: Discounted sum of rewards

	* Infinite vs finite horizon

	* Policy:
		* distribution over actions given state
		* defines the behaviour of the agent
		* MDP policies are time-independent(stationary)

	* Prediction Vs Control
		* Prediction - Given policy, compute the value function for state and state-actions
		* Control - Estimate the value function for state and state-actions for the optimal policy

	* State-value function of an MDP - expected return starting from state s, and then following the policy
	* Action-value function of an MDP - expected return starting from state s, taking action a, and then following policy
	* Optimal state-value function - maximum value function over all policies
	* Optimal action-value function - maximum action-value function over all policies

	* Bellman expectation value
		* Representation as a linear system of equations

	* Optimal Policy
	* Mapping optimal state value function to optimal policy
	* Mapping optimal action value function to optimal policy
	


There are 3 broad strategies for learning behaviour:
	* Rewards
	* Demonstrations
	* Specification of optimal behaviour

* Challenges when learning behaviour:
	* Agent's current actions affects future data
		* Data is sequential and not iid
		* Plain supervised learning would lead to compounding errors
	* Rewards are very far in the future
		* How to attribute credit to actions
	* Performing actions in reality make take time
		* Think autonomous cars
	* How to compose behaviour 